{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f650b99d0b8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return np.array(x/255)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return np.eye(10)[x]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, [None, *image_shape], name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    label_input = tf.placeholder(tf.float32, shape=[None,10],name='y')\n",
    "    \n",
    "    return label_input\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "    \n",
    "    \n",
    "    return keep_prob\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_channel_depth = int(x_tensor.get_shape()[3])\n",
    "    filter_weights = tf.Variable(tf.truncated_normal([*conv_ksize, input_channel_depth, conv_num_outputs],mean=0.0,stddev=0.1,dtype=tf.float32))\n",
    "    bias_weights = tf.Variable(tf.constant(0, shape=[conv_num_outputs], dtype=tf.float32))\n",
    "    \n",
    "    conv_layer = tf.nn.conv2d(input=x_tensor, filter=filter_weights, strides=[1, *conv_strides, 1], padding='SAME')\n",
    "    conv_layer += bias_weights\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    conv_layer = tf.nn.max_pool(conv_layer, [1, *pool_ksize, 1], strides=[1, *pool_strides, 1], padding='SAME')\n",
    "    return conv_layer  \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    fully = tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "    return fully\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    fully = tf.contrib.layers.fully_connected(x_tensor, num_outputs,activation_fn=None)\n",
    "    return fully\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    x_tensor = x\n",
    "    conv_num_outputs = [64,80,96]\n",
    "    conv_ksize = (5,5)\n",
    "    conv_strides = (3,3)\n",
    "    pool_ksize = (2,2)\n",
    "    pool_strides = (2,2)\n",
    "    num_outputs = 10\n",
    "    keep_prob = 0.5\n",
    "    \n",
    "    conv1 = conv2d_maxpool(x_tensor, conv_num_outputs[0], conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv2 = conv2d_maxpool(conv1, conv_num_outputs[1], conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv3 = conv2d_maxpool(conv2, conv_num_outputs[2], conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    #print(tf.shape(conv3))\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat = flatten(conv3)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fc1 = fully_conn(flat, 512)\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    fc2 = fully_conn(fc1, 512)\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    output_data = output(fc2, num_outputs)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return output_data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={\n",
    "            x: feature_batch,\n",
    "            y: label_batch,\n",
    "            keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "     # Calculate batch loss and accuracy\n",
    "    loss = sess.run(cost, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.})\n",
    "    valid_acc = sess.run(accuracy, feed_dict={\n",
    "        x: valid_features,\n",
    "        y: valid_labels,\n",
    "        keep_prob: 1.})\n",
    "\n",
    "    print('Loss: {:>10.4f} Accuracy: {:.6f}'.format(loss,valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.1297 Accuracy: 0.310200\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.9622 Accuracy: 0.350000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.5845 Accuracy: 0.421600\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.2759 Accuracy: 0.447600\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.1651 Accuracy: 0.468000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.0522 Accuracy: 0.470600\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     0.9931 Accuracy: 0.482400\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     0.9728 Accuracy: 0.481000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     0.7444 Accuracy: 0.491400\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     0.7562 Accuracy: 0.486200\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     0.6516 Accuracy: 0.486600\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     0.5979 Accuracy: 0.504800\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     0.5651 Accuracy: 0.497200\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     0.4161 Accuracy: 0.462600\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     0.3490 Accuracy: 0.469400\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     0.4161 Accuracy: 0.447400\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     0.3029 Accuracy: 0.473800\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     0.3871 Accuracy: 0.456600\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     0.3145 Accuracy: 0.486000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     0.2501 Accuracy: 0.479800\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     0.2369 Accuracy: 0.496200\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     0.2346 Accuracy: 0.482400\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.2093 Accuracy: 0.482000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.1660 Accuracy: 0.500200\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.1666 Accuracy: 0.480200\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.1713 Accuracy: 0.473000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.1590 Accuracy: 0.485400\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.1562 Accuracy: 0.480000\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.0798 Accuracy: 0.479400\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.1123 Accuracy: 0.481400\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.1865 Accuracy: 0.479000\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.1173 Accuracy: 0.487800\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.1237 Accuracy: 0.478800\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.1222 Accuracy: 0.473600\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.0810 Accuracy: 0.473600\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.0920 Accuracy: 0.486400\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.0727 Accuracy: 0.477000\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.0778 Accuracy: 0.484200\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.1432 Accuracy: 0.459800\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.0913 Accuracy: 0.466800\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.0784 Accuracy: 0.478800\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.0682 Accuracy: 0.468200\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.0732 Accuracy: 0.478200\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.0848 Accuracy: 0.483000\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.0477 Accuracy: 0.474000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.0433 Accuracy: 0.488200\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.0385 Accuracy: 0.496000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.0429 Accuracy: 0.488000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.0190 Accuracy: 0.480000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.0619 Accuracy: 0.487000\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.0200 Accuracy: 0.490000\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.0478 Accuracy: 0.496400\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.0149 Accuracy: 0.496200\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.0449 Accuracy: 0.483200\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.0591 Accuracy: 0.463000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.0485 Accuracy: 0.458600\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.0168 Accuracy: 0.495000\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.0207 Accuracy: 0.483800\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.0128 Accuracy: 0.477800\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.0058 Accuracy: 0.482000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.0114 Accuracy: 0.488800\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.0208 Accuracy: 0.486800\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.0095 Accuracy: 0.489600\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.0273 Accuracy: 0.494200\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.0148 Accuracy: 0.499800\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.0220 Accuracy: 0.493400\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.0062 Accuracy: 0.485600\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.0028 Accuracy: 0.488600\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.0119 Accuracy: 0.500200\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.0395 Accuracy: 0.489600\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.0116 Accuracy: 0.487600\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.0213 Accuracy: 0.487600\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.0103 Accuracy: 0.493000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.0019 Accuracy: 0.488600\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.0069 Accuracy: 0.493600\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.0047 Accuracy: 0.500200\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.0222 Accuracy: 0.495600\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.0024 Accuracy: 0.499000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.0011 Accuracy: 0.477200\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.0042 Accuracy: 0.501400\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.0042 Accuracy: 0.498400\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.0170 Accuracy: 0.492400\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.0165 Accuracy: 0.502400\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.0079 Accuracy: 0.495400\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.0031 Accuracy: 0.494200\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.0162 Accuracy: 0.495800\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.0053 Accuracy: 0.494400\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.0007 Accuracy: 0.507400\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.0037 Accuracy: 0.488000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.0006 Accuracy: 0.498400\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.0018 Accuracy: 0.499200\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.0010 Accuracy: 0.487200\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.0128 Accuracy: 0.507800\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.0058 Accuracy: 0.503800\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.0134 Accuracy: 0.498600\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.0025 Accuracy: 0.495800\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.0063 Accuracy: 0.501600\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.0018 Accuracy: 0.491200\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.0015 Accuracy: 0.491400\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.0015 Accuracy: 0.489600\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.1166 Accuracy: 0.284800\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     1.7799 Accuracy: 0.374800\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     1.4425 Accuracy: 0.417200\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.5820 Accuracy: 0.435800\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.6397 Accuracy: 0.443800\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.7019 Accuracy: 0.473600\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.3315 Accuracy: 0.499200\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.1261 Accuracy: 0.486200\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.3181 Accuracy: 0.510200\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.2877 Accuracy: 0.530600\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.4431 Accuracy: 0.536600\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.1261 Accuracy: 0.533000\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     0.9060 Accuracy: 0.522600\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.0483 Accuracy: 0.542600\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.0928 Accuracy: 0.555800\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.2560 Accuracy: 0.562000\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.0157 Accuracy: 0.547400\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     0.7739 Accuracy: 0.555800\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     0.9178 Accuracy: 0.567200\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     0.9260 Accuracy: 0.578400\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.1231 Accuracy: 0.577400\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     0.9021 Accuracy: 0.558800\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     0.6998 Accuracy: 0.564800\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     0.8709 Accuracy: 0.584200\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     0.8142 Accuracy: 0.579600\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     0.9600 Accuracy: 0.574400\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     0.7470 Accuracy: 0.580400\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     0.6522 Accuracy: 0.571000\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     0.7271 Accuracy: 0.586200\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     0.6612 Accuracy: 0.580600\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     0.8092 Accuracy: 0.585000\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     0.6935 Accuracy: 0.588000\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     0.5396 Accuracy: 0.579800\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     0.7797 Accuracy: 0.593600\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     0.6174 Accuracy: 0.579600\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     0.7616 Accuracy: 0.594400\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     0.6475 Accuracy: 0.593600\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     0.3891 Accuracy: 0.593800\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     0.6796 Accuracy: 0.606200\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     0.5279 Accuracy: 0.570400\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     0.5971 Accuracy: 0.585000\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     0.6630 Accuracy: 0.580800\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     0.3318 Accuracy: 0.592000\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     0.6969 Accuracy: 0.594600\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     0.3831 Accuracy: 0.597400\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     0.6858 Accuracy: 0.592200\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     0.4800 Accuracy: 0.603400\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     0.3216 Accuracy: 0.592800\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     0.5655 Accuracy: 0.595200\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     0.3903 Accuracy: 0.606400\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     0.4347 Accuracy: 0.583000\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     0.3780 Accuracy: 0.604200\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     0.2648 Accuracy: 0.597200\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     0.3883 Accuracy: 0.603200\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     0.3686 Accuracy: 0.607000\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     0.5187 Accuracy: 0.599200\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     0.4043 Accuracy: 0.616000\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     0.2751 Accuracy: 0.608800\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     0.4311 Accuracy: 0.607000\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     0.2653 Accuracy: 0.612000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     0.5010 Accuracy: 0.609200\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     0.3405 Accuracy: 0.613600\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     0.2317 Accuracy: 0.609400\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     0.3690 Accuracy: 0.594200\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     0.3948 Accuracy: 0.597000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     0.4358 Accuracy: 0.607600\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     0.3188 Accuracy: 0.602200\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     0.2665 Accuracy: 0.601800\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     0.3805 Accuracy: 0.589200\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     0.3382 Accuracy: 0.589800\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     0.4807 Accuracy: 0.600400\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     0.3678 Accuracy: 0.583400\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     0.1830 Accuracy: 0.615600\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     0.3766 Accuracy: 0.593800\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     0.2978 Accuracy: 0.595000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     0.3524 Accuracy: 0.601600\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     0.2434 Accuracy: 0.573400\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     0.2393 Accuracy: 0.591200\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     0.3390 Accuracy: 0.592200\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     0.2537 Accuracy: 0.589800\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     0.3314 Accuracy: 0.595200\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     0.2358 Accuracy: 0.590000\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     0.1895 Accuracy: 0.589400\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     0.3320 Accuracy: 0.585800\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     0.2604 Accuracy: 0.594400\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     0.2987 Accuracy: 0.593000\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     0.3254 Accuracy: 0.591000\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     0.1619 Accuracy: 0.584200\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     0.2273 Accuracy: 0.608200\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     0.1507 Accuracy: 0.595000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     0.3828 Accuracy: 0.581200\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     0.1961 Accuracy: 0.597000\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     0.1613 Accuracy: 0.592600\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     0.2713 Accuracy: 0.601000\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     0.1540 Accuracy: 0.606400\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     0.2813 Accuracy: 0.587200\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     0.2073 Accuracy: 0.607200\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     0.1632 Accuracy: 0.602000\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     0.2064 Accuracy: 0.610400\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     0.1070 Accuracy: 0.598200\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     0.2986 Accuracy: 0.599600\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     0.1319 Accuracy: 0.601400\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     0.1083 Accuracy: 0.590600\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     0.2353 Accuracy: 0.602800\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     0.1061 Accuracy: 0.597000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     0.2568 Accuracy: 0.593800\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     0.1937 Accuracy: 0.609000\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     0.0814 Accuracy: 0.581200\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     0.1694 Accuracy: 0.596800\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     0.1379 Accuracy: 0.585400\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.2684 Accuracy: 0.593400\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     0.1673 Accuracy: 0.602200\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     0.0872 Accuracy: 0.586600\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     0.1604 Accuracy: 0.601200\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     0.1065 Accuracy: 0.591400\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.2712 Accuracy: 0.590600\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     0.1716 Accuracy: 0.592200\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     0.0933 Accuracy: 0.593600\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     0.1282 Accuracy: 0.590800\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     0.1489 Accuracy: 0.581400\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.2811 Accuracy: 0.586800\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     0.1209 Accuracy: 0.596000\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     0.0937 Accuracy: 0.603000\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     0.1717 Accuracy: 0.594200\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     0.1147 Accuracy: 0.607400\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.3139 Accuracy: 0.578400\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     0.1446 Accuracy: 0.596400\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     0.1075 Accuracy: 0.602200\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     0.2199 Accuracy: 0.591000\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     0.1086 Accuracy: 0.598600\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.2456 Accuracy: 0.577400\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     0.1843 Accuracy: 0.598400\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     0.1334 Accuracy: 0.599200\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     0.1733 Accuracy: 0.593400\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     0.0813 Accuracy: 0.613000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.2260 Accuracy: 0.581800\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     0.1378 Accuracy: 0.592000\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     0.1195 Accuracy: 0.587800\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     0.1863 Accuracy: 0.582000\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     0.0819 Accuracy: 0.607800\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.2450 Accuracy: 0.599800\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     0.1144 Accuracy: 0.599000\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     0.0823 Accuracy: 0.591600\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     0.1497 Accuracy: 0.576600\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     0.0886 Accuracy: 0.599800\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.2172 Accuracy: 0.588000\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     0.1664 Accuracy: 0.578200\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     0.0927 Accuracy: 0.595000\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     0.1355 Accuracy: 0.574800\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     0.0822 Accuracy: 0.597200\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.2777 Accuracy: 0.595200\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     0.0672 Accuracy: 0.587400\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     0.0733 Accuracy: 0.589200\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     0.1631 Accuracy: 0.585000\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     0.0683 Accuracy: 0.604600\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.1945 Accuracy: 0.590200\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     0.1170 Accuracy: 0.579400\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     0.0671 Accuracy: 0.606200\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     0.1434 Accuracy: 0.575600\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     0.0642 Accuracy: 0.602200\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.2266 Accuracy: 0.595200\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     0.0571 Accuracy: 0.580200\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     0.0769 Accuracy: 0.603400\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     0.1640 Accuracy: 0.593200\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     0.0491 Accuracy: 0.598400\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.1812 Accuracy: 0.595200\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     0.0788 Accuracy: 0.594400\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     0.0921 Accuracy: 0.590000\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     0.0661 Accuracy: 0.588800\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     0.0807 Accuracy: 0.590400\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.1596 Accuracy: 0.596600\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     0.0908 Accuracy: 0.593000\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     0.0593 Accuracy: 0.596000\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     0.1044 Accuracy: 0.585400\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     0.0468 Accuracy: 0.579800\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.1865 Accuracy: 0.589200\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     0.0847 Accuracy: 0.590400\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     0.0739 Accuracy: 0.601400\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     0.0931 Accuracy: 0.581400\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     0.0673 Accuracy: 0.575200\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.1783 Accuracy: 0.590800\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     0.0608 Accuracy: 0.598800\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     0.1141 Accuracy: 0.583800\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     0.0939 Accuracy: 0.589600\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     0.0511 Accuracy: 0.582400\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.1736 Accuracy: 0.591800\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     0.0732 Accuracy: 0.593400\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     0.0650 Accuracy: 0.591200\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     0.0785 Accuracy: 0.590400\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     0.0619 Accuracy: 0.589200\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.1511 Accuracy: 0.591600\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     0.0849 Accuracy: 0.587800\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     0.0845 Accuracy: 0.585400\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     0.0587 Accuracy: 0.577400\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     0.0351 Accuracy: 0.584800\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.1634 Accuracy: 0.590600\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     0.0548 Accuracy: 0.596800\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     0.1198 Accuracy: 0.595400\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     0.0666 Accuracy: 0.585000\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     0.0473 Accuracy: 0.593800\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.1595 Accuracy: 0.588200\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.0503 Accuracy: 0.591400\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     0.0423 Accuracy: 0.588400\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     0.0530 Accuracy: 0.577600\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     0.0337 Accuracy: 0.593800\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.1105 Accuracy: 0.591400\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     0.0538 Accuracy: 0.583400\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.0845 Accuracy: 0.595400\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     0.0413 Accuracy: 0.591600\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     0.0545 Accuracy: 0.589400\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.1961 Accuracy: 0.581600\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     0.0549 Accuracy: 0.584200\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.0391 Accuracy: 0.592600\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     0.0870 Accuracy: 0.598200\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     0.0505 Accuracy: 0.599400\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.1798 Accuracy: 0.589400\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.0616 Accuracy: 0.579800\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.0783 Accuracy: 0.594600\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     0.0605 Accuracy: 0.592400\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     0.0263 Accuracy: 0.594400\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.1110 Accuracy: 0.590400\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     0.0410 Accuracy: 0.577600\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.0375 Accuracy: 0.596200\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     0.0517 Accuracy: 0.579000\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     0.0342 Accuracy: 0.589600\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.2879 Accuracy: 0.576800\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     0.0457 Accuracy: 0.579200\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.0540 Accuracy: 0.598800\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     0.0802 Accuracy: 0.587800\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     0.0248 Accuracy: 0.594200\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.1215 Accuracy: 0.595800\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.0356 Accuracy: 0.585200\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.0545 Accuracy: 0.592600\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     0.0919 Accuracy: 0.587800\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     0.0348 Accuracy: 0.593800\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.1208 Accuracy: 0.585800\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.0932 Accuracy: 0.580400\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.0461 Accuracy: 0.597200\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     0.0681 Accuracy: 0.587800\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     0.0519 Accuracy: 0.591600\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.1190 Accuracy: 0.584600\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.0420 Accuracy: 0.584800\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.0550 Accuracy: 0.594800\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     0.1106 Accuracy: 0.581400\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     0.0712 Accuracy: 0.592000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.0846 Accuracy: 0.591800\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.0287 Accuracy: 0.578800\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.0275 Accuracy: 0.584400\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     0.0472 Accuracy: 0.583400\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     0.0167 Accuracy: 0.591800\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.1083 Accuracy: 0.587600\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.0726 Accuracy: 0.580000\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.0515 Accuracy: 0.594800\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     0.0822 Accuracy: 0.583800\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     0.0226 Accuracy: 0.590200\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.0596 Accuracy: 0.597200\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.0378 Accuracy: 0.585600\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.0195 Accuracy: 0.584600\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     0.0581 Accuracy: 0.581400\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     0.0364 Accuracy: 0.603800\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.0447 Accuracy: 0.596600\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.0372 Accuracy: 0.581800\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.0230 Accuracy: 0.588000\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     0.1037 Accuracy: 0.583600\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     0.0167 Accuracy: 0.590200\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.0798 Accuracy: 0.584200\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.0176 Accuracy: 0.597600\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.0431 Accuracy: 0.587600\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     0.1039 Accuracy: 0.578400\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     0.0190 Accuracy: 0.583800\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.0724 Accuracy: 0.587000\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.0203 Accuracy: 0.580200\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.0133 Accuracy: 0.589000\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     0.0574 Accuracy: 0.579000\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     0.0343 Accuracy: 0.588200\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.1131 Accuracy: 0.576800\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.0427 Accuracy: 0.586600\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.0601 Accuracy: 0.585600\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     0.0639 Accuracy: 0.584000\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     0.0673 Accuracy: 0.586000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.1317 Accuracy: 0.571000\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.0641 Accuracy: 0.584000\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.0337 Accuracy: 0.590000\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     0.0468 Accuracy: 0.576400\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     0.1223 Accuracy: 0.574800\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.0833 Accuracy: 0.592000\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.0179 Accuracy: 0.581800\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.0200 Accuracy: 0.591000\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     0.1083 Accuracy: 0.577800\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     0.0375 Accuracy: 0.581400\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.1780 Accuracy: 0.584000\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.0246 Accuracy: 0.584600\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.0461 Accuracy: 0.582600\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     0.0502 Accuracy: 0.569800\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     0.0217 Accuracy: 0.588400\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.0849 Accuracy: 0.599200\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.0504 Accuracy: 0.582000\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.0314 Accuracy: 0.584000\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.0471 Accuracy: 0.588000\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     0.0408 Accuracy: 0.582800\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.1521 Accuracy: 0.592200\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.0199 Accuracy: 0.585800\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.0393 Accuracy: 0.590400\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     0.0717 Accuracy: 0.569800\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     0.0429 Accuracy: 0.587600\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.0582 Accuracy: 0.588000\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.0255 Accuracy: 0.567000\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.0431 Accuracy: 0.588600\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     0.0397 Accuracy: 0.580400\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     0.0092 Accuracy: 0.582600\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.1234 Accuracy: 0.594600\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.0165 Accuracy: 0.580000\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.0350 Accuracy: 0.594200\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.0256 Accuracy: 0.581400\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     0.0258 Accuracy: 0.577800\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.1066 Accuracy: 0.596200\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.0128 Accuracy: 0.596000\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.0148 Accuracy: 0.599200\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.0265 Accuracy: 0.579000\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     0.0439 Accuracy: 0.589400\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.0444 Accuracy: 0.596200\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.0100 Accuracy: 0.578600\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.0128 Accuracy: 0.593400\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.0520 Accuracy: 0.588000\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     0.0423 Accuracy: 0.586800\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.0793 Accuracy: 0.596800\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.0206 Accuracy: 0.565800\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.0193 Accuracy: 0.588800\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.0329 Accuracy: 0.576400\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     0.0232 Accuracy: 0.588600\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.1297 Accuracy: 0.589800\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.0142 Accuracy: 0.573600\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.0173 Accuracy: 0.595200\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.0315 Accuracy: 0.572200\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     0.0159 Accuracy: 0.595000\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.0425 Accuracy: 0.598400\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.0221 Accuracy: 0.581200\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.0691 Accuracy: 0.597400\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.0400 Accuracy: 0.574200\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     0.0170 Accuracy: 0.588600\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.0515 Accuracy: 0.592800\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.0215 Accuracy: 0.578400\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.0523 Accuracy: 0.589800\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.0356 Accuracy: 0.573400\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     0.0129 Accuracy: 0.592200\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.0672 Accuracy: 0.596400\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.0500 Accuracy: 0.583800\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.0397 Accuracy: 0.590800\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.0255 Accuracy: 0.573000\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     0.0150 Accuracy: 0.593600\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.0823 Accuracy: 0.589000\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.0604 Accuracy: 0.579400\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.0465 Accuracy: 0.588000\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.0166 Accuracy: 0.580800\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     0.0061 Accuracy: 0.587600\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.0448 Accuracy: 0.596200\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.0145 Accuracy: 0.572400\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.0290 Accuracy: 0.593800\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.0440 Accuracy: 0.590600\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     0.0096 Accuracy: 0.587000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.0273 Accuracy: 0.595000\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.0084 Accuracy: 0.578800\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.0132 Accuracy: 0.583000\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.0275 Accuracy: 0.586600\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     0.0065 Accuracy: 0.592800\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.0282 Accuracy: 0.590000\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.0293 Accuracy: 0.563600\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.0184 Accuracy: 0.596200\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.0245 Accuracy: 0.593800\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     0.0428 Accuracy: 0.589600\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.0280 Accuracy: 0.585600\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.0032 Accuracy: 0.584400\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.0234 Accuracy: 0.586800\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.0197 Accuracy: 0.587600\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.0122 Accuracy: 0.587800\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.0251 Accuracy: 0.591800\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.0194 Accuracy: 0.582600\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.0038 Accuracy: 0.592000\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.0137 Accuracy: 0.586200\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     0.0460 Accuracy: 0.587200\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.0692 Accuracy: 0.594000\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.0193 Accuracy: 0.587000\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.0121 Accuracy: 0.576800\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.0199 Accuracy: 0.575600\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     0.0075 Accuracy: 0.587000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.0360 Accuracy: 0.589400\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.0068 Accuracy: 0.582000\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.0077 Accuracy: 0.584600\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.0333 Accuracy: 0.573200\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     0.0067 Accuracy: 0.588400\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.0226 Accuracy: 0.588000\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.0150 Accuracy: 0.585600\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.0053 Accuracy: 0.589400\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.0190 Accuracy: 0.584000\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.0158 Accuracy: 0.589400\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.0243 Accuracy: 0.580800\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.0197 Accuracy: 0.584800\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.0271 Accuracy: 0.581800\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.0058 Accuracy: 0.593800\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.0171 Accuracy: 0.586000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.0469 Accuracy: 0.590000\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.0117 Accuracy: 0.598400\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.0103 Accuracy: 0.590600\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.0063 Accuracy: 0.582000\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     0.0256 Accuracy: 0.587800\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.0371 Accuracy: 0.589000\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.0050 Accuracy: 0.580800\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.0166 Accuracy: 0.586400\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.0080 Accuracy: 0.588600\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     0.0114 Accuracy: 0.584200\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.0176 Accuracy: 0.584800\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.0970 Accuracy: 0.585000\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.0286 Accuracy: 0.597200\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.0094 Accuracy: 0.585800\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.0151 Accuracy: 0.581200\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.0388 Accuracy: 0.586200\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.0820 Accuracy: 0.595600\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.0068 Accuracy: 0.583200\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.0057 Accuracy: 0.583200\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.0187 Accuracy: 0.579600\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.0151 Accuracy: 0.590800\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.0061 Accuracy: 0.591800\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.0184 Accuracy: 0.580000\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.0217 Accuracy: 0.589400\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.0066 Accuracy: 0.574200\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.0292 Accuracy: 0.587600\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.0182 Accuracy: 0.592200\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.0232 Accuracy: 0.580000\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.0100 Accuracy: 0.591000\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.0085 Accuracy: 0.585200\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.0092 Accuracy: 0.587400\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.0238 Accuracy: 0.595200\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.0085 Accuracy: 0.586200\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.0251 Accuracy: 0.585000\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.0024 Accuracy: 0.582600\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.0501 Accuracy: 0.585600\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.0134 Accuracy: 0.588600\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.0049 Accuracy: 0.588600\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.0042 Accuracy: 0.588000\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.0214 Accuracy: 0.587400\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.0363 Accuracy: 0.576200\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.0057 Accuracy: 0.592000\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.0231 Accuracy: 0.580000\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.0179 Accuracy: 0.593200\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.0363 Accuracy: 0.577800\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.0146 Accuracy: 0.581200\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.0067 Accuracy: 0.589400\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.0180 Accuracy: 0.576400\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.0157 Accuracy: 0.594000\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.0029 Accuracy: 0.579400\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.0239 Accuracy: 0.586200\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.0178 Accuracy: 0.590200\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.0076 Accuracy: 0.581400\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.0220 Accuracy: 0.580000\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.0080 Accuracy: 0.578200\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.0214 Accuracy: 0.580800\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.0398 Accuracy: 0.586800\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.0016 Accuracy: 0.590200\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.0018 Accuracy: 0.589600\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.0455 Accuracy: 0.578400\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.0161 Accuracy: 0.578000\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.0053 Accuracy: 0.590600\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.0074 Accuracy: 0.575200\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.0137 Accuracy: 0.588400\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.0194 Accuracy: 0.582600\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.0080 Accuracy: 0.585200\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.0112 Accuracy: 0.589000\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.0083 Accuracy: 0.588800\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.0099 Accuracy: 0.591600\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.0149 Accuracy: 0.582600\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.0067 Accuracy: 0.586200\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.0245 Accuracy: 0.590800\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.0218 Accuracy: 0.595400\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.0044 Accuracy: 0.586200\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.0214 Accuracy: 0.590200\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.0056 Accuracy: 0.586000\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.0219 Accuracy: 0.592400\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.0028 Accuracy: 0.584000\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.0067 Accuracy: 0.582200\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.0178 Accuracy: 0.583400\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.0761 Accuracy: 0.584600\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.0057 Accuracy: 0.587200\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.0462 Accuracy: 0.595200\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.0269 Accuracy: 0.586800\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.0550 Accuracy: 0.592400\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.0258 Accuracy: 0.575800\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.0145 Accuracy: 0.589200\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.0076 Accuracy: 0.594800\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.0038 Accuracy: 0.597200\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.0671 Accuracy: 0.591400\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.0420 Accuracy: 0.585400\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.0341 Accuracy: 0.587800\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.0168 Accuracy: 0.589800\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     0.0236 Accuracy: 0.587000\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.0023 Accuracy: 0.590000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.0099 Accuracy: 0.584400\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.0066 Accuracy: 0.593000\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.0150 Accuracy: 0.588600\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.0059 Accuracy: 0.593800\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.0108 Accuracy: 0.588600\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.5897943037974683\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH8Psbh+RPUu+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f64aac73710>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
